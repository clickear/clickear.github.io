<!DOCTYPE html>
<html><head><title>∑ mq</title><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta property="og:title" content="∑ mq"/><meta property="og:description" content="关键词 § 异步、削峰、解耦、消息分发、大事务拆分成 (小事务+ 异步)、错误重试、消费者幂、分布式、重复消费、消息丢失、顺序消费等 什么是消息队列(MQ)? § 队列是一种先进先出的数据结构。但是作为中间件，跟单机中的队列是不一样的。 毫无疑问，单机的队列是无法满足需求的，集群/分布式。 消除了单点故障、保证消息可靠性等。消息队列可以简单理解为：把要传输的数据放在队列中。消息队列做业务解耦/最终一致性/广播/错峰流控等。反之，如果需要强一致性，关注业务逻辑的处理结果，则RPC显得更为合适。 名称定义 § 消费者: 生产者 broker 优点 § 解耦 § 作为一个订单系统，我们可以需要在创建订单、有库存信息或者有物流信息调用其它子系统。耦合严重。如某个系统挂了，导致该接口异常。将消息发送到队列中，需要使用的系统，去订阅的方式 异步 § 非必要的操作，异步去处理。而不是同步。比如下单支付完之后，发送等短信，发送短信的结果不应该影响 用户下单，这样异步化，可以提供接口qps。当然，异步的方式有很多，比如你开启线程池处理等。 削峰 § 在秒杀或者有活动的时候，通过先把请求放到队列里面，然后至于每秒消费多少请求，就看自己的服务器处理能力，你能处理5000QPS你就消费这么多，可能会比正常的慢一点，但是不至于打挂服务器。等活动结束之后，可以慢慢消费队列中的数据。比如3更半夜去消费等。比如常见的压单等操作 缺点 § 数据一致性 § 下单的服务自己保证自己的逻辑成功处理了，你成功发了消息，但是优惠券系统，积分系统等等这么多系统，他们成功还是失败你就不管了？这里就涉及到分布式事务、最终一致性等问题。 可用性 § 复杂度增加，要考虑 分布式服务的可用性。 系统复杂度提高 § 硬生生加个 MQ 进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。 适用场景 § 消息队列做业务解耦/最终一致性/广播/错峰流控等。反之，如果需要强一致性，关注业务逻辑的处理结果，则RPC显得更为合适。 上游不关心下游的执行结果 § 生产者不需要从消费者处获得反馈。如同步数据、发送通知。 容许短暂的不一致性 § 技术选型对比 § 综上，各种对比之后，有如下建议： 一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了； 后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高； 不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 RocketMQ与Kafka对比 § 数据可靠性 § RocketMQ支持异步实时刷盘，同步刷盘，同步Replication，异步Replication Kafka使用异步刷盘方式，异步Replication/同步Replication RocketMQ的同步刷盘在单机可靠性上比Kafka更高，不会因为操作系统Crash，导致数据丢失。 Kafka同步Replication理论上性能低于RocketMQ的同步Replication，原因是Kafka的数据以分区为单位组织，意味着一个Kafka实例上会有几百个数据分区，RocketMQ一个实例上只有一个数据分区，RocketMQ可以充分利用IO Group Commit机制，批量传输数据，配置同步Replication与异步Replication相比，性能损耗约20%~30% 性能对比 § Kafka单机写入TPS约在百万条/秒，消息大小10个字节 RocketMQ单机写入TPS单实例约7万条/秒，单机部署3个Broker，可以跑到最高12万条/秒，消息大小10个字节 Kafka的TPS跑到单机百万，主要是由于Producer端将多个小消息合并，批量发向Broker。 RocketMQ为什么没有这么做？ Producer通常使用Java语言，缓存过多消息，GC是个很严重的问题 Producer调用发送消息接口，消息未发送到Broker，向业务返回成功，此时Producer宕机，会导致大量消息丢失，业务出错 Producer通常为分布式系统，且每台机器都是多线程发送，我们认为线上的系统单个Producer每秒产生的数据量有限，不可能上万。 缓存的功能完全可以由上层业务完成。 单机支持的队列数 § Kafka单机超过64个队列/分区，Load会发生明显的飙高现象，队列越多，load越高，发送消息响应时间变长。 RocketMQ单机支持最高5万个队列，Load不会发生明显变化 队列多有什么好处？ 单机可以创建更多Topic，因为每个Topic都是由一批队列组成 Consumer的集群规模和队列数成正比，队列越多，Consumer集群可以越大 严格的消息顺序 § Kafka支持消息顺序，但是一台Broker宕机后，就会产生消息乱序 RocketMQ支持严格的消息顺序，在顺序消息场景下，一台Broker宕机后，发送消息会失败，但是不会乱序 分布式事务消息 § Kafka支持分布式事务消息 RocketMQ事务消息是支持的 但是两者对事务的区分度不一样。 RocketMQ 解决的是本地事务的执行和发消息这两个动作满足事务的约束。 Kafka 事务消息则是用在一次事务中需要发送多个消息的情况，保证多个消息之间的事务约束，即多条消息要么都发送成功，要么都发送失败。 消息查询 § Kafka不支持消息查询 现在应该是支持的。具体未调研 RocketMQ支持根据Message Id查询消息，也支持根据消息内容查询消息（发送消息时指定一个Message Key，任意字符串，例如指定为订单Id） 高性能的存储 § Kafka在Topic数量由64增长到256时，吞吐量下降了98."/><meta property="og:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="og:width" content="1200"/><meta property="og:height" content="675"/><link rel="icon" href="./static/icon.png"/><meta name="description" content="关键词 § 异步、削峰、解耦、消息分发、大事务拆分成 (小事务+ 异步)、错误重试、消费者幂、分布式、重复消费、消息丢失、顺序消费等 什么是消息队列(MQ)? § 队列是一种先进先出的数据结构。但是作为中间件，跟单机中的队列是不一样的。 毫无疑问，单机的队列是无法满足需求的，集群/分布式。 消除了单点故障、保证消息可靠性等。消息队列可以简单理解为：把要传输的数据放在队列中。消息队列做业务解耦/最终一致性/广播/错峰流控等。反之，如果需要强一致性，关注业务逻辑的处理结果，则RPC显得更为合适。 名称定义 § 消费者: 生产者 broker 优点 § 解耦 § 作为一个订单系统，我们可以需要在创建订单、有库存信息或者有物流信息调用其它子系统。耦合严重。如某个系统挂了，导致该接口异常。将消息发送到队列中，需要使用的系统，去订阅的方式 异步 § 非必要的操作，异步去处理。而不是同步。比如下单支付完之后，发送等短信，发送短信的结果不应该影响 用户下单，这样异步化，可以提供接口qps。当然，异步的方式有很多，比如你开启线程池处理等。 削峰 § 在秒杀或者有活动的时候，通过先把请求放到队列里面，然后至于每秒消费多少请求，就看自己的服务器处理能力，你能处理5000QPS你就消费这么多，可能会比正常的慢一点，但是不至于打挂服务器。等活动结束之后，可以慢慢消费队列中的数据。比如3更半夜去消费等。比如常见的压单等操作 缺点 § 数据一致性 § 下单的服务自己保证自己的逻辑成功处理了，你成功发了消息，但是优惠券系统，积分系统等等这么多系统，他们成功还是失败你就不管了？这里就涉及到分布式事务、最终一致性等问题。 可用性 § 复杂度增加，要考虑 分布式服务的可用性。 系统复杂度提高 § 硬生生加个 MQ 进来，你怎么保证消息没有重复消费？怎么处理消息丢失的情况？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。 适用场景 § 消息队列做业务解耦/最终一致性/广播/错峰流控等。反之，如果需要强一致性，关注业务逻辑的处理结果，则RPC显得更为合适。 上游不关心下游的执行结果 § 生产者不需要从消费者处获得反馈。如同步数据、发送通知。 容许短暂的不一致性 § 技术选型对比 § 综上，各种对比之后，有如下建议： 一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了； 后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高； 不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 Apache，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。 所以中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。 如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 RocketMQ与Kafka对比 § 数据可靠性 § RocketMQ支持异步实时刷盘，同步刷盘，同步Replication，异步Replication Kafka使用异步刷盘方式，异步Replication/同步Replication RocketMQ的同步刷盘在单机可靠性上比Kafka更高，不会因为操作系统Crash，导致数据丢失。 Kafka同步Replication理论上性能低于RocketMQ的同步Replication，原因是Kafka的数据以分区为单位组织，意味着一个Kafka实例上会有几百个数据分区，RocketMQ一个实例上只有一个数据分区，RocketMQ可以充分利用IO Group Commit机制，批量传输数据，配置同步Replication与异步Replication相比，性能损耗约20%~30% 性能对比 § Kafka单机写入TPS约在百万条/秒，消息大小10个字节 RocketMQ单机写入TPS单实例约7万条/秒，单机部署3个Broker，可以跑到最高12万条/秒，消息大小10个字节 Kafka的TPS跑到单机百万，主要是由于Producer端将多个小消息合并，批量发向Broker。 RocketMQ为什么没有这么做？ Producer通常使用Java语言，缓存过多消息，GC是个很严重的问题 Producer调用发送消息接口，消息未发送到Broker，向业务返回成功，此时Producer宕机，会导致大量消息丢失，业务出错 Producer通常为分布式系统，且每台机器都是多线程发送，我们认为线上的系统单个Producer每秒产生的数据量有限，不可能上万。 缓存的功能完全可以由上层业务完成。 单机支持的队列数 § Kafka单机超过64个队列/分区，Load会发生明显的飙高现象，队列越多，load越高，发送消息响应时间变长。 RocketMQ单机支持最高5万个队列，Load不会发生明显变化 队列多有什么好处？ 单机可以创建更多Topic，因为每个Topic都是由一批队列组成 Consumer的集群规模和队列数成正比，队列越多，Consumer集群可以越大 严格的消息顺序 § Kafka支持消息顺序，但是一台Broker宕机后，就会产生消息乱序 RocketMQ支持严格的消息顺序，在顺序消息场景下，一台Broker宕机后，发送消息会失败，但是不会乱序 分布式事务消息 § Kafka支持分布式事务消息 RocketMQ事务消息是支持的 但是两者对事务的区分度不一样。 RocketMQ 解决的是本地事务的执行和发消息这两个动作满足事务的约束。 Kafka 事务消息则是用在一次事务中需要发送多个消息的情况，保证多个消息之间的事务约束，即多条消息要么都发送成功，要么都发送失败。 消息查询 § Kafka不支持消息查询 现在应该是支持的。具体未调研 RocketMQ支持根据Message Id查询消息，也支持根据消息内容查询消息（发送消息时指定一个Message Key，任意字符串，例如指定为订单Id） 高性能的存储 § Kafka在Topic数量由64增长到256时，吞吐量下降了98."/><meta name="generator" content="Quartz"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link href="./index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap" rel="stylesheet" type="text/css" spa-preserve/><script src="./prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch(`./static/contentIndex.json`).then(data => data.json())</script></head><body data-slug="∑-mq"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h1 class="page-title"><a href=".">🪴 Quartz 4.0</a></h1><div class="spacer mobile-only"></div><div class="search"><div id="search-icon"><p>Search</p><div></div><svg tabIndex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search</title><desc id="desc">Search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></div><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="results-container"></div></div></div></div><div class="darkmode"><input class="toggle" id="darkmode-toggle" type="checkbox" tabIndex="-1"/><label id="toggle-label-light" for="darkmode-toggle" tabIndex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlnsXlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35;" xmlSpace="preserve"><title>Light mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg></label><label id="toggle-label-dark" for="darkmode-toggle" tabIndex="-1"><svg xmlns="http://www.w3.org/2000/svg" xmlnsXlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'" xmlSpace="preserve"><title>Dark mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></label></div><div class="toc desktop-only"><button type="button" id="toc"><h3>Table of Contents</h3><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="fold"><polyline points="6 9 12 15 18 9"></polyline></svg></button><div id="toc-content"><ul class="overflow"><li class="depth-0"><a href="#关键词" data-for="关键词">关键词</a></li><li class="depth-0"><a href="#什么是消息队列mq" data-for="什么是消息队列mq">什么是消息队列(MQ)?</a></li><li class="depth-1"><a href="#名称定义" data-for="名称定义">名称定义</a></li><li class="depth-1"><a href="#优点" data-for="优点">优点</a></li><li class="depth-1"><a href="#缺点" data-for="缺点">缺点</a></li><li class="depth-0"><a href="#适用场景" data-for="适用场景">适用场景</a></li><li class="depth-1"><a href="#上游不关心下游的执行结果" data-for="上游不关心下游的执行结果">上游不关心下游的执行结果</a></li><li class="depth-0"><a href="#技术选型对比" data-for="技术选型对比">技术选型对比</a></li><li class="depth-1"><a href="#rocketmq与kafka对比" data-for="rocketmq与kafka对比">RocketMQ与Kafka对比</a></li><li class="depth-1"><a href="#为什么kafaka的topic吞吐量会下降" data-for="为什么kafaka的topic吞吐量会下降">为什么kafaka的topic，吞吐量会下降？</a></li><li class="depth-0"><a href="#mq的高可用" data-for="mq的高可用">mq的高可用</a></li><li class="depth-1"><a href="#rocketmq" data-for="rocketmq">rocketmq</a></li><li class="depth-1"><a href="#kafaka" data-for="kafaka">kafaka</a></li><li class="depth-0"><a href="#消息丢失" data-for="消息丢失">消息丢失</a></li><li class="depth-0"><a href="#消息重复-幂等处理" data-for="消息重复-幂等处理">消息重复: 幂等处理</a></li><li class="depth-0"><a href="#消息积压" data-for="消息积压">消息积压</a></li><li class="depth-1"><a href="#提高消费并行度" data-for="提高消费并行度">提高消费并行度</a></li><li class="depth-1"><a href="#批量方式消费" data-for="批量方式消费">批量方式消费</a></li><li class="depth-1"><a href="#跳过非重要消息" data-for="跳过非重要消息">跳过非重要消息</a></li><li class="depth-1"><a href="#优化每条消息消费过程" data-for="优化每条消息消费过程">优化每条消息消费过程</a></li><li class="depth-0"><a href="#顺序性" data-for="顺序性">顺序性</a></li></ul></div></div></div><div class="center"><div class="page-header"><div class="popover-hint"><h1 class="article-title">∑ mq</h1><p class="content-meta">Oct 08, 2023, 22 min read</p></div></div><article class="popover-hint"><h2 id="关键词">关键词<a aria-hidden="true" tabindex="-1" href="#关键词" class="internal"> §</a></h2>
<p>异步、削峰、解耦、消息分发、大事务拆分成 <code>(小事务+ 异步)</code>、错误重试、消费者幂、分布式、<strong>重复消费、消息丢失、顺序消费</strong>等</p>
<h2 id="什么是消息队列mq">什么是消息队列(MQ)?<a aria-hidden="true" tabindex="-1" href="#什么是消息队列mq" class="internal"> §</a></h2>
<blockquote>
<p>队列是一种先进先出的数据结构。但是作为中间件，跟单机中的队列是不一样的。 毫无疑问，单机的队列是无法满足需求的，集群/分布式。 消除了单点故障、保证消息可靠性等。消息队列可以简单理解为：把要传输的数据放在队列中。消息队列做业务解耦/最终一致性/广播/错峰流控等。反之，如果需要强一致性，关注业务逻辑的处理结果，则RPC显得更为合适。</p>
</blockquote>
<h3 id="名称定义">名称定义<a aria-hidden="true" tabindex="-1" href="#名称定义" class="internal"> §</a></h3>
<ul>
<li>消费者:</li>
<li>生产者</li>
<li>broker</li>
</ul>
<h3 id="优点">优点<a aria-hidden="true" tabindex="-1" href="#优点" class="internal"> §</a></h3>
<h4 id="解耦">解耦<a aria-hidden="true" tabindex="-1" href="#解耦" class="internal"> §</a></h4>
<blockquote>
<p>作为一个订单系统，我们可以需要在创建订单、有库存信息或者有物流信息调用其它子系统。耦合严重。如某个系统挂了，导致该接口异常。将消息发送到队列中，需要使用的系统，去订阅的方式<br/>
<img src="http://image.clickear.top/20220919091014.png" alt/></p>
</blockquote>
<h4 id="异步">异步<a aria-hidden="true" tabindex="-1" href="#异步" class="internal"> §</a></h4>
<blockquote>
<p>非必要的操作，异步去处理。而不是同步。比如下单支付完之后，发送等短信，发送短信的结果不应该影响 用户下单，这样异步化，可以提供接口qps。当然，异步的方式有很多，比如你开启线程池处理等。<br/>
<img src="http://image.clickear.top/20220919091047.png" alt/></p>
</blockquote>
<h4 id="削峰">削峰<a aria-hidden="true" tabindex="-1" href="#削峰" class="internal"> §</a></h4>
<blockquote>
<p>在秒杀或者有活动的时候，通过先把请求放到队列里面，然后至于每秒消费多少请求，就看自己的服务器处理能力，你能处理5000QPS你就消费这么多，可能会比正常的慢一点，但是不至于打挂服务器。等活动结束之后，可以慢慢消费队列中的数据。比如3更半夜去消费等。比如常见的压单等操作</p>
</blockquote>
<h3 id="缺点">缺点<a aria-hidden="true" tabindex="-1" href="#缺点" class="internal"> §</a></h3>
<h4 id="数据一致性">数据一致性<a aria-hidden="true" tabindex="-1" href="#数据一致性" class="internal"> §</a></h4>
<blockquote>
<p>下单的服务自己保证自己的逻辑成功处理了，你成功发了消息，但是优惠券系统，积分系统等等这么多系统，他们成功还是失败你就不管了？这里就涉及到<a href="./分布式事务" class="internal">分布式事务</a>、<a href="./最终一致性" class="internal">最终一致性</a>等问题。</p>
</blockquote>
<h4 id="可用性">可用性<a aria-hidden="true" tabindex="-1" href="#可用性" class="internal"> §</a></h4>
<blockquote>
<p>复杂度增加，要考虑 分布式服务的可用性。</p>
</blockquote>
<h4 id="系统复杂度提高">系统复杂度提高<a aria-hidden="true" tabindex="-1" href="#系统复杂度提高" class="internal"> §</a></h4>
<blockquote>
<p>硬生生加个 MQ 进来，你怎么<a href="https://github.com/doocs/advanced-java/blob/main/docs/high-concurrency/how-to-ensure-that-messages-are-not-repeatedly-consumed.md" class="external">保证消息没有重复消费</a>？怎么<a href="https://github.com/doocs/advanced-java/blob/main/docs/high-concurrency/how-to-ensure-the-reliable-transmission-of-messages.md" class="external">处理消息丢失的情况</a>？怎么保证消息传递的顺序性？头大头大，问题一大堆，痛苦不已。</p>
</blockquote>
<h2 id="适用场景">适用场景<a aria-hidden="true" tabindex="-1" href="#适用场景" class="internal"> §</a></h2>
<blockquote>
<p>消息队列做业务解耦/最终一致性/广播/错峰流控等。反之，如果需要强一致性，关注业务逻辑的处理结果，则RPC显得更为合适。</p>
</blockquote>
<h3 id="上游不关心下游的执行结果">上游不关心下游的执行结果<a aria-hidden="true" tabindex="-1" href="#上游不关心下游的执行结果" class="internal"> §</a></h3>
<p>生产者不需要从消费者处获得反馈。如同步数据、发送通知。</p>
<h4 id="容许短暂的不一致性">容许短暂的不一致性<a aria-hidden="true" tabindex="-1" href="#容许短暂的不一致性" class="internal"> §</a></h4>
<h2 id="技术选型对比"><a href="https://github.com/doocs/advanced-java/blob/main/docs/high-concurrency/why-mq.md" class="external">技术选型对比</a><a aria-hidden="true" tabindex="-1" href="#技术选型对比" class="internal"> §</a></h2>
<p><img src="http://image.clickear.top/20220919091854.png" alt/><br/>
综上，各种对比之后，有如下建议：</p>
<p>一般的业务系统要引入 MQ，最早大家都用 ActiveMQ，但是现在确实大家用的不多了，没经过大规模吞吐量场景的验证，社区也不是很活跃，所以大家还是算了吧，我个人不推荐用这个了；</p>
<p>后来大家开始用 RabbitMQ，但是确实 erlang 语言阻止了大量的 Java 工程师去深入研究和掌控它，对公司而言，几乎处于不可控的状态，但是确实人家是开源的，比较稳定的支持，活跃度也高；</p>
<p>不过现在确实越来越多的公司会去用 RocketMQ，确实很不错，毕竟是阿里出品，但社区可能有突然黄掉的风险（目前 RocketMQ 已捐给 <a href="https://github.com/apache/rocketmq" class="external">Apache</a>，但 GitHub 上的活跃度其实不算高）对自己公司技术实力有绝对自信的，推荐用 RocketMQ，否则回去老老实实用 RabbitMQ 吧，人家有活跃的开源社区，绝对不会黄。</p>
<p>所以<strong>中小型公司</strong>，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；<strong>大型公司</strong>，基础架构研发实力较强，用 RocketMQ 是很好的选择。</p>
<p>如果是<strong>大数据领域</strong>的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。</p>
<h3 id="rocketmq与kafka对比">RocketMQ与Kafka对比<a aria-hidden="true" tabindex="-1" href="#rocketmq与kafka对比" class="internal"> §</a></h3>
<h4 id="数据可靠性">数据可靠性<a aria-hidden="true" tabindex="-1" href="#数据可靠性" class="internal"> §</a></h4>
<ul>
<li>RocketMQ支持异步实时刷盘，同步刷盘，同步Replication，异步Replication</li>
<li>Kafka使用异步刷盘方式，异步Replication/同步Replication</li>
</ul>
<p>RocketMQ的同步刷盘在单机可靠性上比Kafka更高，不会因为操作系统Crash，导致数据丢失。</p>
<p>Kafka同步Replication理论上性能低于RocketMQ的同步Replication，原因是Kafka的数据以分区为单位组织，意味着一个Kafka实例上会有几百个数据分区，RocketMQ一个实例上只有一个数据分区，RocketMQ可以充分利用IO Group Commit机制，批量传输数据，配置同步Replication与异步Replication相比，性能损耗约20%~30%</p>
<h4 id="性能对比">性能对比<a aria-hidden="true" tabindex="-1" href="#性能对比" class="internal"> §</a></h4>
<ul>
<li>Kafka单机写入TPS约在百万条/秒，消息大小10个字节</li>
<li>RocketMQ单机写入TPS单实例约7万条/秒，单机部署3个Broker，可以跑到最高12万条/秒，消息大小10个字节</li>
</ul>
<p>Kafka的TPS跑到单机百万，主要是由于Producer端将多个小消息合并，批量发向Broker。</p>
<p>RocketMQ为什么没有这么做？</p>
<ol>
<li>Producer通常使用Java语言，缓存过多消息，GC是个很严重的问题</li>
<li>Producer调用发送消息接口，消息未发送到Broker，向业务返回成功，此时Producer宕机，会导致大量消息丢失，业务出错</li>
<li>Producer通常为分布式系统，且每台机器都是多线程发送，我们认为线上的系统单个Producer每秒产生的数据量有限，不可能上万。</li>
<li>缓存的功能完全可以由上层业务完成。</li>
</ol>
<h4 id="单机支持的队列数">单机支持的队列数<a aria-hidden="true" tabindex="-1" href="#单机支持的队列数" class="internal"> §</a></h4>
<ul>
<li>Kafka单机超过64个队列/分区，Load会发生明显的飙高现象，队列越多，load越高，发送消息响应时间变长。</li>
<li>RocketMQ单机支持最高5万个队列，Load不会发生明显变化</li>
</ul>
<p>队列多有什么好处？</p>
<p>单机可以创建更多Topic，因为每个Topic都是由一批队列组成</p>
<p>Consumer的集群规模和队列数成正比，队列越多，Consumer集群可以越大</p>
<h4 id="严格的消息顺序">严格的消息顺序<a aria-hidden="true" tabindex="-1" href="#严格的消息顺序" class="internal"> §</a></h4>
<ul>
<li>Kafka支持消息顺序，但是一台Broker宕机后，就会产生消息乱序</li>
<li>RocketMQ支持严格的消息顺序，在顺序消息场景下，一台Broker宕机后，发送消息会失败，但是不会乱序</li>
</ul>
<h4 id="分布式事务消息">分布式事务消息<a aria-hidden="true" tabindex="-1" href="#分布式事务消息" class="internal"> §</a></h4>
<ul>
<li>Kafka支持分布式事务消息</li>
<li><a href="./RocketMQ事务消息" class="internal">RocketMQ事务消息</a>是支持的<br/>
但是两者对事务的区分度不一样。</li>
</ul>
<ul>
<li>RocketMQ 解决的是<strong>本地事务的执行和发消息这两个动作满足事务</strong>的约束。</li>
<li>Kafka 事务消息则是用<strong>在一次事务中需要发送多个消息</strong>的情况，保证多个消息之间的事务约束，即多条消息要么都发送成功，要么都发送失败。</li>
</ul>
<h4 id="消息查询">消息查询<a aria-hidden="true" tabindex="-1" href="#消息查询" class="internal"> §</a></h4>
<ul>
<li><del>Kafka不支持消息查询</del> 现在应该是支持的。具体未调研</li>
<li>RocketMQ支持根据Message Id查询消息，也支持根据消息内容查询消息（发送消息时指定一个Message Key，任意字符串，例如指定为订单Id）</li>
</ul>
<h4 id="高性能的存储">高性能的存储<a aria-hidden="true" tabindex="-1" href="#高性能的存储" class="internal"> §</a></h4>
<ul>
<li>Kafka在Topic数量由64增长到256时，吞吐量下降了98.37%。</li>
<li>RocketMQ在Topic数量由64增长到256时，吞吐量只下降了16%。</li>
</ul>
<ol>
<li>kafka一个topic下面的所有消息都是以partition的方式分布式的存储在多个节点上。同时在kafka的机器上，<strong>每个Partition</strong>其实都会对应一个日志目录，在目录下面会对应多个日志分段。所以如果Topic很多的时候Kafka虽然写文件是顺序写，但实际上文件过多，会造成<strong>磁盘IO竞争</strong>非常激烈。</li>
<li>rocketmq消息主体数据并没有像Kafka一样写入多个文件，而是<strong>写入一个文件</strong>,这样我们的写入IO竞争就非常小，可以在很多Topic的时候依然保持很高的吞吐量。虽然ConsumeQueue写是在不停的写入，并且ConsumeQueue是以Queue维度来创建文件，文件数量依然很多，但是<strong>ConsumeQueue</strong>的<strong>写入的数据量很小</strong>，每条消息只有20个字节，30W条数据也才6M左右，所以其实对我们的影响相对Kafka的Topic之间影响是要小很多的</li>
</ol>
<h4 id="读取消息">读取消息<a aria-hidden="true" tabindex="-1" href="#读取消息" class="internal"> §</a></h4>
<p>Kafka中每个Partition都会是一个单独的文件，所以当消费某个消息的时候，会很好的出现顺序读，我们知道OS从物理磁盘上访问读取文件的同时，会顺序对其他相邻块的数据文件进行预读取，将数据放入PageCache，所以Kafka的读取消息性能比较好。</p>
<p>RocketMQ读取流程如下：</p>
<ul>
<li>先读取ConsumerQueue中的offset对应CommitLog物理的offset</li>
<li>根据offset读取CommitLog</li>
</ul>
<p>ConsumerQueue也是每个Queue一个单独的文件，并且其文件体积小，所以很容易利用PageCache提高性能。而CommitLog，由于同一个Queue的连续消息在CommitLog其实是不连续的，所以会造成随机读，RocketMQ对此做了几个优化：</p>
<ul>
<li>Mmap映射读取，Mmap的方式减少了传统IO将磁盘文件数据在操作系统内核地址空间的缓冲区和用户应用程序地址空间的缓冲区之间来回进行拷贝的性能开销</li>
<li>使用DeadLine调度算法+SSD存储盘</li>
<li>由于Mmap映射受到内存限制，当不在Mmmap映射这部分数据的时候(也就是消息堆积过多)，默认是内存的40%，会将请求发送到SLAVE,减缓Master的压力</li>
</ul>
<h3 id="为什么kafaka的topic吞吐量会下降">为什么kafaka的topic，吞吐量会下降？<a aria-hidden="true" tabindex="-1" href="#为什么kafaka的topic吞吐量会下降" class="internal"> §</a></h3>
<p><a href="http://www.itpub.net/2019/11/27/4449/" class="external">你应该知道的RocketMQ – ITPUB</a></p>
<ol>
<li>kafka一个topic下面的所有消息都是以partition的方式分布式的存储在多个节点上。同时在kafka的机器上，<strong>每个Partition</strong>其实都会对应一个日志目录，在目录下面会对应多个日志分段。所以如果Topic很多的时候Kafka虽然写文件是顺序写，但实际上文件过多，会造成<strong>磁盘IO竞争</strong>非常激烈。</li>
<li>rocketmq消息主体数据并没有像Kafka一样写入多个文件，而是<strong>写入一个文件</strong>,这样我们的写入IO竞争就非常小，可以在很多Topic的时候依然保持很高的吞吐量。虽然ConsumeQueue写是在不停的写入，并且ConsumeQueue是以Queue维度来创建文件，文件数量依然很多，但是<strong>ConsumeQueue</strong>的<strong>写入的数据量很小</strong>，每条消息只有20个字节，30W条数据也才6M左右，所以其实对我们的影响相对Kafka的Topic之间影响是要小很多的</li>
</ol>
<h2 id="mq的高可用">mq的高可用<a aria-hidden="true" tabindex="-1" href="#mq的高可用" class="internal"> §</a></h2>
<h3 id="rocketmq">rocketmq<a aria-hidden="true" tabindex="-1" href="#rocketmq" class="internal"> §</a></h3>
<p>一个 Topic 分布在多个Broker 上，一个 Broker 可以配置多个 Topic，它们是多对多的关系。如果某个 Topic 消息量很大，应该给它多配置几个队列，并且尽量多分布在不同 Broker 上，以减轻某个 Broker 的压力。Topic 消息量都比较均匀的情况下，如果某个 broker 上的队列越多，则该 broker 压力越大。<a href="https://www.itmuch.com/books/rocketmq/architecture.html" class="external">架构(Architecture) · Apache RocketMQ开发者指南</a></p>
<p><em>写数据</em>的时候，生产者就写 leader，超过半数才算成功。 基于raft协议</p>
<h3 id="kafaka">kafaka<a aria-hidden="true" tabindex="-1" href="#kafaka" class="internal"> §</a></h3>
<p><a href="https://github.com/doocs/advanced-java/blob/main/docs/high-concurrency/how-to-ensure-high-availability-of-message-queues.md" class="external">消息队列高可用</a></p>
<p>Kafka 一个最基本的架构认识：由多个 broker 组成，每个 broker 是一个节点；你创建一个 topic，这个 topic 可以划分为多个 partition，每个 partition 可以存在于不同的 broker 上，每个 partition 就放一部分数据。<br/>
这就是<strong>天然的分布式消息队列</strong>，就是说一个 topic 的数据，是<strong>分散放在多个机器上的，每个机器就放一部分数据</strong><br/>
<strong>写数据</strong>的时候，生产者就写 leader，然后 leader 将数据落地写本地磁盘，接着其他 follower 自己主动从 leader 来 pull 数据。一旦所有 follower 同步好数据了，就会发送 ack 给 leader，leader 收到所有 follower 的 ack 之后，就会返回写成功的消息给生产者。<br/>
在 Kafka 服务端设置 <code>min.insync.replicas</code> 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。<br/>
<strong>消费</strong>的时候，只会从 leader 去读，但是只有当一个消息已经被所有 follower 都同步成功返回 ack 的时候，这个消息才会被消费者读到。</p>
<h2 id="消息丢失">消息丢失<a aria-hidden="true" tabindex="-1" href="#消息丢失" class="internal"> §</a></h2>
<p>怎么<a href="https://github.com/doocs/advanced-java/blob/main/docs/high-concurrency/how-to-ensure-the-reliable-transmission-of-messages.md" class="external">处理消息丢失的情况</a>？</p>
<ul>
<li><strong>消息生产阶段：</strong> 从消息被生产出来，然后提交给 MQ 的过程中，只要能正常收到 MQ Broker 的 ack 确认响应，就表示发送成功，所以只要处理好返回值和异常，这个阶段是不会出现消息丢失的。</li>
<li><strong>消息存储阶段：</strong> 这个阶段一般会直接交给 MQ 消息中间件来保证，但是你要了解它的原理，比如 Broker 会做副本，保证一条消息至少同步两个节点再返回 ack。 <strong>开启持久化、集群 + 数据副本、刷盘机制</strong></li>
<li><strong>消息消费阶段：</strong> 消费端从 Broker 上拉取消息，只要消费端在收到消息后，不立即发送消费确认给 Broker，而是等到执行完业务逻辑后，再发送消费确认，也能保证消息的不丢失。<strong>ACK确认机制</strong></li>
<li>降级补偿机制: 生产者持久化 + 消费者处理回调保障</li>
</ul>





























<table><thead><tr><th></th><th>rocketMq</th><th>kafaka</th><th>rabbitmq</th></tr></thead><tbody><tr><td>生产者</td><td>1. 使用同步发送。<br/>2. 使用分布式事务消息</td><td></td><td></td></tr><tr><td>服务端</td><td>1. 使用同步刷盘策略<br/>2. 使用同步复制机制</td><td></td><td></td></tr><tr><td>消费者</td><td>1. ACK确认机制</td><td></td><td></td></tr></tbody></table>
<h2 id="消息重复-幂等处理">消息重复: 幂等处理<a aria-hidden="true" tabindex="-1" href="#消息重复-幂等处理" class="internal"> §</a></h2>
<p><a href="https://github.com/doocs/advanced-java/blob/main/docs/high-concurrency/how-to-ensure-that-messages-are-not-repeatedly-consumed.md" class="external">保证消息没有重复消费</a>？</p>
<p>由消费者去保障消息重复问题，加操作表(数据库唯一索引或redis等)手段保障</p>
<h2 id="消息积压">消息积压<a aria-hidden="true" tabindex="-1" href="#消息积压" class="internal"> §</a></h2>
<aside> 💡 本着解决线上异常为**最高优先级**，然后通过监控和日志进行排查并优化业务逻辑，最后是扩容消费端和分片的数量。
</aside>
<ol>
<li>保证消费者速度没问题，即先排查bug。</li>
<li>扩容增加消费者
<ol>
<li>如rocketmq,默认是4个MessageQueue。如果当前queue&lt;消费节点数量，则新增消费节点梳理</li>
<li>因queue不能动态新增，所以新增一个比较大queue的topic，将旧topic消息迁移到新的上来。</li>
</ol>
</li>
<li>提升消费进度
<ol>
<li>增加并发，如多线程等</li>
</ol>
</li>
</ol>
<p><a href="https://github.com/doocs/advanced-java/blob/main/docs/high-concurrency/how-to-ensure-that-messages-are-not-repeatedly-consumed.md" class="external">消息积压</a></p>
<h3 id="提高消费并行度">提高消费并行度<a aria-hidden="true" tabindex="-1" href="#提高消费并行度" class="internal"> §</a></h3>
<p>绝大部分消息消费行为都属于 IO 密集型，即可能是操作数据库，或者调用 RPC，这类消费行为的消费速度在于后端数据库或者外系统的吞吐量，通过增加消费并行度，可以提高总的消费吞吐量，但是并行度增加到一定程度，反而会下降。所以，应用必须要设置合理的并行度。 如下有几种修改消费并行度的方法：</p>
<p>同一个 ConsumerGroup 下，通过增加 Consumer 实例数量来提高并行度（需要注意的是超过订阅队列数的 Consumer 实例无效）。可以通过加机器，或者在已有机器启动多个进程的方式。 提高单个 Consumer 的消费并行线程，通过修改参数 consumeThreadMin、consumeThreadMax 实现。</p>
<h3 id="批量方式消费">批量方式消费<a aria-hidden="true" tabindex="-1" href="#批量方式消费" class="internal"> §</a></h3>
<p>某些业务流程如果支持批量方式消费，则可以很大程度上提高消费吞吐量，例如订单扣款类应用，一次处理一个订单耗时 1 s，一次处理 10 个订单可能也只耗时 2 s，这样即可大幅度提高消费的吞吐量，通过设置 consumer 的 consumeMessageBatchMaxSize 返个参数，默认是 1，即一次只消费一条消息，例如设置为 N，那么每次消费的消息数小于等于 N。</p>
<h3 id="跳过非重要消息">跳过非重要消息<a aria-hidden="true" tabindex="-1" href="#跳过非重要消息" class="internal"> §</a></h3>
<p>发生消息堆积时，如果消费速度一直追不上发送速度，如果业务对数据要求不高的话，可以选择丢弃不重要的消息。例如，当某个队列的消息数堆积到 100000 条以上，则尝试丢弃部分或全部消息，这样就可以快速追上发送消息的速度。示例代码如下：</p>
<p><code>public ConsumeConcurrentlyStatus consumeMessage( List&lt;MessageExt> msgs, ConsumeConcurrentlyContext context) { long offset = msgs.get(0).getQueueOffset(); String maxOffset = msgs.get(0).getProperty(Message.PROPERTY_MAX_OFFSET); long diff = Long.parseLong(maxOffset) - offset; if (diff > 100000) { // TODO 消息堆积情况的特殊处理 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } // TODO 正常消费过程 return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; }</code></p>
<h3 id="优化每条消息消费过程">优化每条消息消费过程<a aria-hidden="true" tabindex="-1" href="#优化每条消息消费过程" class="internal"> §</a></h3>
<h2 id="顺序性">顺序性<a aria-hidden="true" tabindex="-1" href="#顺序性" class="internal"> §</a></h2>
<p><a href="https://github.com/doocs/advanced-java/blob/main/docs/high-concurrency/how-to-ensure-the-order-of-messages.md" class="external">advanced-java · GitHub</a></p>
<p>rocketMq如何保证顺序性</p>
<blockquote class="callout" data-callout="tip">
<div class="callout-title">
                  <div class="callout-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M8.5 14.5A2.5 2.5 0 0 0 11 12c0-1.38-.5-2-1-3-1.072-2.143-.224-4.054 2-6 .5 2.5 2 4.9 4 6.5 2 1.6 3 3.5 3 5.5a7 7 0 1 1-14 0c0-1.153.433-2.294 1-3a2.5 2.5 0 0 0 2.5 2.5z"></path></svg></div>
                  <div class="callout-title-inner"><p>整体思路💡 </p></div>
                  
                </div>
<ol>
<li>单一生产者和单一消费者，单一queue</li>
</ol>
</blockquote></article></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1}"></div><svg version="1.1" id="global-graph-icon" xmlns="http://www.w3.org/2000/svg" xmlnsXlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xmlSpace="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
	s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
	c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
	C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
	c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
	v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
	s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
	C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
	S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
	s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
	s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1}"></div></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li>No backlinks found</li></ul></div></div></div><footer><hr/><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.0.10</a>, © 2023</p><ul><li><a href="https://github.com/jackyzha0/quartz">GitHub</a></li><li><a href="https://discord.gg/cRFFHYye7t">Discord Community</a></li></ul></footer></div></body><script type="application/javascript">// quartz/components/scripts/quartz/components/scripts/callout.inline.ts
function toggleCallout() {
  const outerBlock = this.parentElement;
  outerBlock.classList.toggle(`is-collapsed`);
  const collapsed = outerBlock.classList.contains(`is-collapsed`);
  const height = collapsed ? this.scrollHeight : outerBlock.scrollHeight;
  outerBlock.style.maxHeight = height + `px`;
  let current = outerBlock;
  let parent = outerBlock.parentElement;
  while (parent) {
    if (!parent.classList.contains(`callout`)) {
      return;
    }
    const collapsed2 = parent.classList.contains(`is-collapsed`);
    const height2 = collapsed2 ? parent.scrollHeight : parent.scrollHeight + current.scrollHeight;
    parent.style.maxHeight = height2 + `px`;
    current = parent;
    parent = parent.parentElement;
  }
}
function setupCallout() {
  const collapsible = document.getElementsByClassName(
    `callout is-collapsible`
  );
  for (const div of collapsible) {
    const title = div.firstElementChild;
    if (title) {
      title.removeEventListener(`click`, toggleCallout);
      title.addEventListener(`click`, toggleCallout);
      const collapsed = div.classList.contains(`is-collapsed`);
      const height = collapsed ? title.scrollHeight : div.scrollHeight;
      div.style.maxHeight = height + `px`;
    }
  }
}
document.addEventListener(`nav`, setupCallout);
window.addEventListener(`resize`, setupCallout);
</script><script type="module">
          import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.esm.min.mjs';
          const darkMode = document.documentElement.getAttribute('saved-theme') === 'dark'
          mermaid.initialize({
            startOnLoad: false,
            securityLevel: 'loose',
            theme: darkMode ? 'dark' : 'default'
          });
          document.addEventListener('nav', async () => {
            await mermaid.run({
              querySelector: '.mermaid'
            })
          });
          </script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="./postscript.js" type="module"></script></html>